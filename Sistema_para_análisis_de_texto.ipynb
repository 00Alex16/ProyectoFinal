{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sistema para análisis de texto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVoBrHAf25MuQpnjf0Gh1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/00Alex16/ProyectoFinal/blob/main/Sistema_para_an%C3%A1lisis_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-mqLLlD6Uiv"
      },
      "source": [
        "import requests as rq\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from afinn import Afinn\n",
        "\n",
        "# Obtener contenido del archivo desde github\n",
        "raw_url = \"https://raw.githubusercontent.com/00Alex16/ProyectoFinal/main/textos/ejemplo_1.txt\"\n",
        "text = rq.get(raw_url).text\n",
        "\n",
        "# Función para limpiar y tokenizar el texto\n",
        "def process_doc(document):\n",
        "    \n",
        "    new_doc = document.lower()\n",
        "    new_doc = re.sub('http\\S+', ' ', new_doc)\n",
        "    punctuation_marks = '[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
        "    new_doc = re.sub(punctuation_marks , ' ', new_doc)\n",
        "    new_doc = re.sub(\"\\d+\", ' ', new_doc)\n",
        "    new_doc = re.sub(\"\\\\s+\", ' ', new_doc)\n",
        "    new_doc = new_doc.split(sep = ' ')\n",
        "    new_doc = [token for token in new_doc if len(token) > 1]\n",
        "    \n",
        "    return(new_doc)\n",
        "\n",
        "# Guardamos la lista con las palabras relevantes\n",
        "new_text = process_doc(text)\n",
        "\n",
        "# Función para establecer similitudes en el texto\n",
        "def topic_identification(tokens):\n",
        "\n",
        "  stop_words = list(stopwords.words('english'))\n",
        "  filtered_words = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  l = len(filtered_words)\n",
        "  doc = [filtered_words[0:int(l/4)], filtered_words[int(l/4):int(2*l/4)], filtered_words[int(2*l/4):int(3*l/4)], filtered_words[int(3*l/4):]]\n",
        "  dictionary = corpora.Dictionary(doc)\n",
        "  DT_matrix = [dictionary.doc2bow(d) for d in doc]\n",
        "  Lda_object = gensim.models.ldamodel.LdaModel\n",
        "  lda_model_1 = Lda_object(DT_matrix, num_topics=3, id2word = dictionary)\n",
        "  return lda_model_1.print_topics(num_topics=3, num_words=4)\n",
        "\n",
        "# Función para identificar los personajes\n",
        "def characters_identification(tokens):\n",
        "\n",
        "  tags = nltk.pos_tag(tokens)\n",
        "  nouns = [(word, tag) for word, tag in tags if tag in ['NN']]\n",
        "  return nouns\n",
        "\n",
        "# Función para realizar el análisis de sentimientos\n",
        "def sent_analysis(tokens):\n",
        "\n",
        "  analysis = \"\"\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lem_tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "  lem_text = \" \".join(lem_tokens)\n",
        "  afn = Afinn()\n",
        "  if afn.score(lem_text) > 5:\n",
        "    return \"Positivo\"\n",
        "  elif afn.score(lem_text) < -5:\n",
        "    return \"Negativo\"\n",
        "  else:\n",
        "    return \"Neutro\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}